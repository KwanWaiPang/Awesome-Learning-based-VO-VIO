# Paper Survey for Learning-based VO, VIO, and IO
- [Learning-based VO](#Learning-based-VO)
- [Learning-based VIO](#Learning-based-VIO)
- [Learning-based IO](#Learning-based-IO)

## Learning-based VO

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2024|`RAL`|[Efficient Camera Exposure Control for Visual Odometry via Deep Reinforcement Learning](https://arxiv.org/pdf/2408.17005)|[![Github stars](https://img.shields.io/github/stars/ShuyangUni/drl_exposure_ctrl.svg)](https://github.com/ShuyangUni/drl_exposure_ctrl)|---| 
|2024|`ECCV`|[Reinforcement learning meets visual odometry](https://arxiv.org/pdf/2407.15626)|[![Github stars](https://img.shields.io/github/stars/uzh-rpg/rl_vo.svg)](https://github.com/uzh-rpg/rl_vo)|[Blog](https://kwanwaipang.github.io/RL-for-VO/)|
|2024|`IROS`|[Deep Visual Odometry with Events and Frames](https://arxiv.org/pdf/2309.09947)|[![Github stars](https://img.shields.io/github/stars/uzh-rpg/rampvo.svg)](https://github.com/uzh-rpg/rampvo)|---| 
|2024|`3DV`|[Deep event visual odometry](https://arxiv.org/pdf/2312.09800)|[![Github stars](https://img.shields.io/github/stars/tum-vision/DEVO.svg)](https://github.com/tum-vision/DEVO)|---|
|2024|`ECCV`|[Deep patch visual slam](https://arxiv.org/pdf/2408.01654)|[![Github stars](https://img.shields.io/github/stars/princeton-vl/DPVO.svg)](https://github.com/princeton-vl/DPVO)|---|
|2024|`NIPS`|[Deep patch visual odometry](https://proceedings.neurips.cc/paper_files/paper/2023/file/7ac484b0f1a1719ad5be9aa8c8455fbb-Paper-Conference.pdf)|[![Github stars](https://img.shields.io/github/stars/princeton-vl/DPVO.svg)](https://github.com/princeton-vl/DPVO)|---|
|2022|`Neurocomputing`|[DeepAVO: Efficient pose refining with feature distilling for deep Visual Odometry](https://arxiv.org/pdf/2105.09899)|---|---| 
|2021|`CoRL`|[Tartanvo: A generalizable learning-based vo](https://proceedings.mlr.press/v155/wang21h/wang21h.pdf)|[![Github stars](https://img.shields.io/github/stars/castacks/tartanvo.svg)](https://github.com/castacks/tartanvo)|---|
|2021|`NIPS`|[DROID-SLAM: Deep Visual SLAM for Monocular,Stereo, and RGB-D Cameras](https://proceedings.neurips.cc/paper/2021/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf)|[![Github stars](https://img.shields.io/github/stars/princeton-vl/DROID-SLAM.svg)](https://github.com/princeton-vl/DROID-SLAM)|---|
|2021|`CVPR`|[Generalizing to the open world: Deep visual odometry with online adaptation](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Generalizing_to_the_Open_World_Deep_Visual_Odometry_With_Online_CVPR_2021_paper.pdf)|---|---|
|2021|`arXiv`|[DF-VO: What should be learnt for visual odometry?](https://arxiv.org/pdf/2103.00933)|[![Github stars](https://img.shields.io/github/stars/Huangying-Zhan/DF-VO.svg)](https://github.com/Huangying-Zhan/DF-VO)|---|
|2020|`ICRA`|[Visual odometry revisited: What should be learnt?](https://arxiv.org/pdf/1909.09803)|[![Github stars](https://img.shields.io/github/stars/Huangying-Zhan/DF-VO.svg)](https://github.com/Huangying-Zhan/DF-VO)|---|
|2020|`CVPR`|[Towards better generalization: Joint depth-pose learning without posenet](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Towards_Better_Generalization_Joint_Depth-Pose_Learning_Without_PoseNet_CVPR_2020_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/B1ueber2y/TrianFlow.svg)](https://github.com/B1ueber2y/TrianFlow)|---|
|2020|`CVPR`|[Diffposenet: Direct differentiable camera pose estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Parameshwara_DiffPoseNet_Direct_Differentiable_Camera_Pose_Estimation_CVPR_2022_paper.pdf)|---|---| 
|2020|`CVPR`|[D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_D3VO_Deep_Depth_Deep_Pose_and_Deep_Uncertainty_for_Monocular_CVPR_2020_paper.pdf)|---|[website](https://cvg.cit.tum.de/research/vslam/d3vo)| 
|2021|`IJCV`|[Unsupervised scale-consistent depth learning from video](https://arxiv.org/pdf/2105.11610)|[![Github stars](https://img.shields.io/github/stars/JiawangBian/SC-SfMLearner-Release.svg)](https://github.com/JiawangBian/SC-SfMLearner-Release)<br>[![Github stars](https://img.shields.io/github/stars/JiawangBian/sc_depth_pl.svg)](https://github.com/JiawangBian/sc_depth_pl)|---|
|2019|`NIPS`|[Unsupervised scale-consistent depth and ego-motion learning from monocular video](https://proceedings.neurips.cc/paper/2019/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf)|[![Github stars](https://img.shields.io/github/stars/JiawangBian/SC-SfMLearner-Release.svg)](https://github.com/JiawangBian/SC-SfMLearner-Release)<br>[![Github stars](https://img.shields.io/github/stars/JiawangBian/sc_depth_pl.svg)](https://github.com/JiawangBian/sc_depth_pl)|---|
|2019|`ICRA`|[Ganvo: Unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks](https://arxiv.org/pdf/1809.05786)|---|---|
|2019|`CVPR`|[Recurrent neural network for (un-) supervised learning of monocular video visual odometry and depth](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Recurrent_Neural_Network_for_Un-Supervised_Learning_of_Monocular_Video_Visual_CVPR_2019_paper.pdf)|-|-| 
|2019|`ICRA`|[Pose graph optimization for unsupervised monocular visual odometry](https://arxiv.org/pdf/1903.06315)|---|---|
|2018|`CVPR`|[Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/Huangying-Zhan/Depth-VO-Feat.svg)](https://github.com/Huangying-Zhan/Depth-VO-Feat)|---|
|2018|`ICRA`|[Undeepvo: Monocular visual odometry through unsupervised deep learning](https://arxiv.org/pdf/1709.06841)|-|[website](https://senwang.gitlab.io/UnDeepVO/)|
|2017|`ICRA`|[Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks](https://arxiv.org/pdf/1709.08429)|-|[website](https://senwang.gitlab.io/DeepVO/)|
|2017|`CVPR`|[Unsupervised learning of depth and ego-motion from video](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/tinghuiz/SfMLearner.svg)](https://github.com/tinghuiz/SfMLearner)|---|


## Learning-based VIO

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`TRO`|[Airslam: An efficient and illumination-robust point-line visual slam system](https://arxiv.org/pdf/2408.03520)| [![Github stars](https://img.shields.io/github/stars/sair-lab/AirSLAM.svg)](https://github.com/sair-lab/AirSLAM)|---|
|2024|`arXiv`|[SF-Loc: A Visual Mapping and Geo-Localization System based on Sparse Visual Structure Frames](https://arxiv.org/pdf/2412.01500)|[![Github stars](https://img.shields.io/github/stars/GREAT-WHU/SF-Loc.svg)](https://github.com/GREAT-WHU/SF-Loc)|[test](https://kwanwaipang.github.io/SF-Loc/)|
|2024|`arXiv`|[DEIO: Deep Event Inertial Odometry](https://arxiv.org/pdf/2411.03928)|[![Github stars](https://img.shields.io/github/stars/arclab-hku/DEIO.svg)](https://github.com/arclab-hku/DEIO)|[website](https://kwanwaipang.github.io/DEIO/)|
|2024|`CVPR`|[Adaptive vio: Deep visual-inertial odometry with online continual learning](https://openaccess.thecvf.com/content/CVPR2024/papers/Pan_Adaptive_VIO_Deep_Visual-Inertial_Odometry_with_Online_Continual_Learning_CVPR_2024_paper.pdf)|---|---|
|2024|`RAL`|[DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping](https://arxiv.org/pdf/2403.13714)|[![Github stars](https://img.shields.io/github/stars/GREAT-WHU/DBA-Fusion.svg)](https://github.com/GREAT-WHU/DBA-Fusion)|---|
|2024|`ICRA`|[DVI-SLAM: A dual visual inertial SLAM network](https://arxiv.org/pdf/2309.13814)|---|---|
|2023|`ICRA`|[Bamf-slam: bundle adjusted multi-fisheye visual-inertial slam using recurrent field transforms](https://arxiv.org/pdf/2306.01173)|---|---|
|2022|`ECCV`|[Efficient deep visual and inertial odometry with adaptive visual modality selection](https://arxiv.org/pdf/2205.06187)|[![Github stars](https://img.shields.io/github/stars/mingyuyng/Visual-Selective-VIO.svg)](https://github.com/mingyuyng/Visual-Selective-VIO)|---|
|2022|`IEEE/ASME International Conference on Advanced Intelligent Mechatronics`|[A self-supervised, differentiable Kalman filter for uncertainty-aware visual-inertial odometry](https://arxiv.org/pdf/2203.07207)|---|---| 
|2022|`Neural Networks`|[SelfVIO: Self-supervised deep monocular Visualâ€“Inertial Odometry and depth estimation](https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608022X0004X/1-s2.0-S0893608022000752/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJ3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIDf7HUEf9nazwS2yUVVHHU5ta9IwaqCqR4G70MlU9OviAiEAoc8ucMljX0IQjq3lSj9Ml3mFgAaUMiSt5%2BcB7FBKk0QquwUI5v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDDHD%2BXN7CICp9OyoDyqPBSD0I9lzITj59CQt2JNP%2BKYPKhBjhKHML2Is4lIO%2FHZDtLTJ7isDyzL6G24OQ4UhsjkUWT4c1jVXFdKTVeJvRlVHcvb9rf7ICvFRLck%2B%2BD65oqZqNDEgmvMiaQIJPulumaFpsyIQaUeYI4oQ%2B3V%2FiZK3FyvNlqJC5KyzYFwLnaTabqV41bAXCRCgq16hA8jBZP3AssPpCUvokXgjf7aPsLovt%2BzmSwjN1RMFeUojfD7vFJ5b8tVit583ePkxa6qEAdhnac0%2FyKV4AOX99LR%2FtiL9cTujGs%2BZvn7kUnDZbKQOyQHZ7l%2B6IENPC4oKlC1DBNU1tzaiaPVvSnWBP6P9zzjwBf0SMzAYyQQLT7HGZZ%2Fo22AqSGxA1yTBmRo9Phndy7rKNHg30iuJn6yeE3i1gFmbAxU5HtYnC2q80WYHSeWr0fiZy6UfC9qIdwFHc5VWTMKtd8CmSbaA5bHyWBFPlFXqqNXLuyrqDR4APU8Cor66Sf%2BrtDSy2esYV1VZcLpNdAMGAf6xzanaZzgYl4kqzBd858TNDABGRl8QVl3lo58NBH0wu41B7kWcLbhWjpRlTGQyE7cYIrbZc3ebj6Kaqg%2FbpiNJPFD01eQbKBixrQMyG0bH9CeiPoSkeyUYnrglAEimvAsLgVi5HUyDoEpstSbdeI5YaMDi8JG1RU9UsVCN3u97nzva%2FxPTzIMLFtre9fCklWyuZse5jv6ootedGikl7w6y6xN5wJNsx6LKthbPAxHV3Wm%2BwR9AMT4GyEmlk617gS6%2FL0Mn5of0wOJQHbv4jaqG%2FTuD%2BbWzgsJ0H7NwFbMILqGGl9Fyvdm4FcHN7ojSFNOocx%2F2Iz0YwsX9Y6jDsYPWQG%2FoU36BYPiQRccwi9%2FOvgY6sQGDH6s6Qg67mua%2Bme7iNDdtJq1Ow%2BN4V9%2BX3Kl0LqKrn1RhsGYx6Dtc2JcPT%2FlRaR%2Bbou4JE42jIFaVMTN6Ehi9Ek%2FHDO0KVR5IEiIb2MmSvfe%2FL3chnEl%2Fcdy3PKeNsqruPyvQXSRfokI7fL1LlKBpyinag9rFUox8AiArmdrv6fbRKMWWgczicDiPayH09YJljN%2F51JuKUbsos9kbQwGVF3Mtr7Fx53NyKuWdayBeWX8%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250314T052534Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7K5KBNC6%2F20250314%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=70682f25381b91eaf371d9e771798bb854c6812d6315550b270576ba0e56277c&hash=87a249837b3dc4ed8db1643fd5e1fefd11374458ba38778721b104c13db2b0e6&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608022000752&tid=spdf-25547e74-fff9-42bc-860f-5582e1039d19&sid=9433a396701c33442228b653b9eae1da34edgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f15575150515758015152&rr=920159e76ba341e7&cc=us)|---|---|
|2021|`International Conference on International Joint Conferences on Artificial Intelligence`|[Unsupervised monocular visual-inertial odometry network](https://www.ijcai.org/proceedings/2020/0325.pdf)|[![Github stars](https://img.shields.io/github/stars/Ironbrotherstyle/UnVIO.svg)](https://github.com/Ironbrotherstyle/UnVIO)|---|
|2021|`IEEE International Conference on Acoustics, Speech and Signal Processing `|[Atvio: Attention guided visual-inertial odometry](https://github.com/KwanWaiPang/Awesome-Learning-based-VO-VIO/blob/pdf/file/ATVIO_Attention_Guided_Visual-Inertial_Odometry.pdf)|---|---|
|2019|`TPAMI`|[Unsupervised deep visual-inertial odometry with online error correction for RGB-D imagery](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691513)|---|---|
|2019|`CVPR`|[Selective sensor fusion for neural visual-inertial odometry](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Selective_Sensor_Fusion_for_Neural_Visual-Inertial_Odometry_CVPR_2019_paper.pdf)|---|---|
|2019|`IROS`|[Deepvio: Self-supervised deep learning of monocular visual inertial odometry using 3d geometric constraints](https://arxiv.org/pdf/1906.11435)|---|---| 
|2017|`AAAI`|[Vinet: Visual-inertial odometry as a sequence-to-sequence learning problem](https://arxiv.org/pdf/1701.08376)|[![Github stars](https://img.shields.io/github/stars/HTLife/VINet.svg)](https://github.com/HTLife/VINet)|non-official implementation|


## Learning-based IO

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[AirIO: Learning Inertial Odometry with Enhanced IMU Feature Observability](https://arxiv.org/pdf/2501.15659)|[![Github stars](https://img.shields.io/github/stars/Air-IO/Air-IO.svg)](https://github.com/Air-IO/Air-IO)|[website](https://air-io.github.io/)<br>[Test](https://kwanwaipang.github.io/AirIO/)|
|2024|`TIV`|[Deep learning for inertial positioning: A survey](https://arxiv.org/pdf/2303.03757)|---|---|
|2023|`arXiv`|[End-to-end deep learning framework for real-time inertial attitude estimation using 6dof imu](https://www.researchgate.net/profile/Arman-Asgharpoor-Golroudbari/publication/368469434_END-TO-END_DEEP_LEARNING_FRAMEWORK_FOR_REAL-TIME_INERTIAL_ATTITUDE_ESTIMATION_USING_6DOF_IMU/links/63ea42cebd7860764364396a/End-to-End-Deep-Learning-Framework-for-Real-Time-Inertial-Attitude-Estimation-using-6DoF-IMU.pdf)|---|---|
|2023|`arXiv`|[AirIMU: Learning uncertainty propagation for inertial odometry](https://arxiv.org/pdf/2310.04874)|[![Github stars](https://img.shields.io/github/stars/haleqiu/AirIMU.svg)](https://github.com/haleqiu/AirIMU)|[website](https://airimu.github.io/)<br>[Test](https://kwanwaipang.github.io/AirIMU/)|
|2023|`RAL`|[Learned inertial odometry for autonomous drone racing](https://arxiv.org/pdf/2210.15287)|[![Github stars](https://img.shields.io/github/stars/uzh-rpg/learned_inertial_model_odometry.svg)](https://github.com/uzh-rpg/learned_inertial_model_odometry)|---|
|2022|`ICRA`|[Improved state propagation through AI-based pre-processing and down-sampling of high-speed inertial data](https://www.aau.at/wp-content/uploads/2022/03/imu_preprocessing.pdf)|---|---|
|2022|`RAL`|[Deep IMU Bias Inference for Robust Visual-Inertial Odometry with Factor Graphs](https://arxiv.org/pdf/2211.04517)|---|---|
|2021|`ICRA`|[IMU Data Processing For Inertial Aided Navigation:A Recurrent Neural Network Based Approach](https://arxiv.org/pdf/2103.14286)|---|---|



## Other Resource
* [Awesome-Transformer-based-SLAM](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM)

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2020|`CVPR`|[Superglue: Learning feature matching with graph neural networks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sarlin_SuperGlue_Learning_Feature_Matching_With_Graph_Neural_Networks_CVPR_2020_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/magicleap/SuperGluePretrainedNetwork.svg)](https://github.com/magicleap/SuperGluePretrainedNetwork)|---|
|2020|`IROS`|[Tartanair: A dataset to push the limits of visual slam](https://arxiv.org/pdf/2003.14338)|[![Github stars](https://img.shields.io/github/stars/castacks/tartanair_tools.svg)](https://github.com/castacks/tartanair_tools)|[website](https://theairlab.org/tartanair-dataset/)|
|2019|`CVPR`|[Projective manifold gradient layer for deep rotation regression](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Projective_Manifold_Gradient_Layer_for_Deep_Rotation_Regression_CVPR_2022_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/JYChen18/RPMG.svg)](https://github.com/JYChen18/RPMG)|Better learn rotations|
|2018|`CVPR`|[Superpoint: Self-supervised interest point detection and description](https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/rpautrat/SuperPoint.svg)](https://github.com/rpautrat/SuperPoint)|[pytorch version](https://github.com/eric-yyjau/pytorch-superpoint)[event-based superpoint](https://github.com/mingyip/pytorch-superpoint)|




